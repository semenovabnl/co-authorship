{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm \n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBRegressor\n",
    "import collections\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from unidecode import unidecode \n",
    "# from scipy.spatial.distance import euclidean, cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import re\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "# from gensim.models.fasttext import FastText\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = stopwords.words('english')\n",
    "from collections import Counter,defaultdict\n",
    "# from gensim.models.fasttext import load_facebook_model\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge embbeding functions\n",
    "def avg_sum(v1, v2):\n",
    "    return (np.array(v1)+np.array(v2))/2\n",
    "def mult(v1, v2):\n",
    "    return np.array(v1)*np.array(v2)\n",
    "def w_l1(v1, v2):\n",
    "    return np.abs(np.array(v1)-np.array(v2))\n",
    "def w_l2(v1, v2):\n",
    "    return (np.array(v1)-np.array(v2))**2\n",
    "def nw_l1(v1, v2, graph, n1, n2, embs ):\n",
    "    neig1 = [n for n in graph.neighbors(n1)]\n",
    "    neig2 = [n for n in graph.neighbors(n2)]\n",
    "    sum1 = np.zeros(len(v1))\n",
    "    sum2 = np.zeros(len(v2))\n",
    "    for n in neig1:\n",
    "        sum1 += np.array(embs[int(n)])\n",
    "    for n in neig2:\n",
    "        sum2 += np.array(embs[int(n)])\n",
    "    return np.abs((sum1+np.array(v1))/(len(neig1)+1)+(sum2+np.array(v2))/(len(neig2)+1))\n",
    "def nw_l2(v1, v2, graph, n1, n2, embs ):\n",
    "    neig1 = [n for n in graph.neighbors(n1)]\n",
    "    neig2 = [n for n in graph.neighbors(n2)]\n",
    "    sum1 = np.zeros(len(v1))\n",
    "    sum2 = np.zeros(len(v2))\n",
    "    for n in neig1:\n",
    "        sum1 += np.array(embs[int(n)])\n",
    "    for n in neig2:\n",
    "        sum2 += np.array(embs[int(n)])\n",
    "    return ((sum1+np.array(v1))/(len(neig1)+1)+(sum2+np.array(v2))/(len(neig2)+1))**2\n",
    "def unknown_key_pressed(emb1, emb2):\n",
    "    print('Нажата неизвестная клавиша')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "def eud_cos(v1, v2):\n",
    "    eud = euclidean_distances(v1, v2)\n",
    "    cos = cosine_distances(v1, v2)\n",
    "    return np.array((eud[0][0],cos[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators_4 = ['avg_sum', 'mult', 'w_l1', 'w_l2']\n",
    "operators_2 = ['nw_l1', 'nw_l2']\n",
    "# cosine cosine= {1: avg_sum, 2: mult, 3: w_l1, 4: w_l2}\n",
    "# operators_2 = {1: nw_l1, 2: nw_l2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "Graph_train = nx.read_weighted_edgelist(\"train_test/edge_list_binary_train_2018.txt\", delimiter=' ',nodetype=int)\n",
    "Graph_test_2019 = nx.read_weighted_edgelist(\"train_test/edges_2019_binary.txt\", delimiter=' ',nodetype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка ребер с негатив семплинг\n",
    "f_train = open('train_test/edge_list_train_NegSamp.txt')\n",
    "f_2019 = open('train_test/edge_list_2019_NegSamp.txt')\n",
    "\n",
    "edges_train = []\n",
    "edges_2019 = []\n",
    "\n",
    "for line in f_train:\n",
    "    edges_train.append(line.split(' '))\n",
    "for line in f_2019:\n",
    "    edges_2019.append(line.split(' '))\n",
    "f_train.close()\n",
    "f_2019.close()\n",
    "\n",
    "for i in range(len(edges_train)):\n",
    "    edges_train[i][2] = edges_train[i][2][:-1]\n",
    "\n",
    "for i in range(len(edges_2019)):\n",
    "    edges_2019[i][2] = edges_2019[i][2][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_train_dict = {(e[0], e[1]) : e[2] for e in edges_train}\n",
    "edges_2019_dict = {(e[0], e[1]) : e[2] for e in edges_2019}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_train = []\n",
    "for e in edges_train:\n",
    "    nodes_train.extend(e[:2]) # экстендим парами и убираем дубликаты вершин\n",
    "nodes_train = set(nodes_train)\n",
    "\n",
    "nodes_2019 = []\n",
    "for e in edges_2019:\n",
    "    nodes_2019.extend(e[:2])\n",
    "nodes_2019 = set(nodes_2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_rf(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # RandomForest\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_pred_train = rf.predict(X_train)\n",
    "    y_pred_test = rf.predict(X_test)\n",
    "    \n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    f1_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    f1_micro_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_micro_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    logloss_train = log_loss(y_train, y_pred_train)\n",
    "    logloss_test = log_loss(y_test, y_pred_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    return precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_gbc(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # GradientBoosting\n",
    "    params = {'n_estimators': 100, 'max_depth': 4, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.01}\n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "                     \n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    \n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    f1_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    f1_micro_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_micro_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    logloss_train = log_loss(y_train, y_pred_train)\n",
    "    logloss_test = log_loss(y_test, y_pred_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    return precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_svm(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # SVC\n",
    "#     y_train = label_binarize(y_train, classes=[0, 1, 2, 3, 4])\n",
    "    svm_model = svm.SVC()\n",
    "#     svm_model = svm.SVC(probability=True)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "#     print(X_train, \"X_train\")\n",
    "#     print(y_train, 'y_train')\n",
    "                     \n",
    "    y_pred_train = svm_model.predict(X_train)\n",
    "    y_pred_test = svm_model.predict(X_test)\n",
    "#     y_pred_train_proba = svm_model.predict_proba(X_train)\n",
    "#     y_pred_test_proba = svm_model.predict_proba(X_test)\n",
    "#     print(y_pred_train_proba[0])\n",
    "\n",
    "#     print(y_pred_train, 'y_pred_train')\n",
    "#     labels = [0,1]\n",
    "    \n",
    "    precision_train = precision_score(y_train, y_pred_train, average=None)\n",
    "    precision_test = precision_score(y_test, y_pred_test, average=None)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    f1_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    f1_micro_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_micro_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    logloss_train = log_loss(y_train, y_pred_train)\n",
    "    logloss_test = log_loss(y_test, y_pred_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    return precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_xgb(X_train, y_train, X_test, y_test):\n",
    "#     param = {\n",
    "#    'max_depth': 3,\n",
    "#    'eta': 0.3, \n",
    "#    'silent': 1, \n",
    "#    'objective': 'multi:softprob',\n",
    "#    'num_class': 2}\n",
    "#     num_round = 20\n",
    "    param = {'max_depth': 2, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic'}\n",
    "    num_round = 10\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "#     clf = xgboost.XGBClassifier()\n",
    "    clf = xgb.train(param, dtrain, num_round)\n",
    "#     clf = XGBRegressor()\n",
    "#     clf.fit(X_train, y_train, verbose=False)\n",
    "    evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "    clf = xgb.train(param, dtrain, num_round)\n",
    "    \n",
    "#     clf.fit(X_train, y_train)\n",
    "#     predictions = [round(value) for value in y_pred]\n",
    "    y_pred_train_prob = clf.predict(dtrain)\n",
    "    y_pred_train = [int(round(value)) for value in y_pred_train_prob]\n",
    "    y_pred_test_prob = clf.predict(dtest)\n",
    "    y_pred_test = [int(round(value)) for value in y_pred_test_prob]\n",
    "#     print(y_pred_test)\n",
    "    \n",
    "#     y_pred_train = clf.predict(X_train)\n",
    "#     y_pred_train = np.asarray([np.argmax(line) for line in y_pred_train])\n",
    "#     print(y_pred_train[0])\n",
    "#     y_pred_test = clf.predict(X_test)\n",
    "#     y_pred_test = np.asarray([np.argmax(line) for line in y_pred_test])\n",
    "\n",
    "    \n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    f1_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    f1_micro_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_micro_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    logloss_train = log_loss(y_train, y_pred_train_prob)\n",
    "    logloss_test = log_loss(y_test, y_pred_test_prob)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    return precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = ['model_rf', 'model_xgb', 'model_gbc', 'model_svm']\n",
    "functions = {1: model_rf, 2: model_xgb}\n",
    "#3: model_gbc, 4: model_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for operator in range(len(operators_4)+1)[1:]:\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    # f_emb_train = open('./Data/emb_done/out_2018.txt')\n",
    "    f_emb = open('./Data/emb_done/out.txt')\n",
    "    emb = []\n",
    "    # emb_2018 = []\n",
    "    for line in f_emb:\n",
    "        emb.append(line.split(' '))\n",
    "    # for line in f_emb_2018:\n",
    "    #     emb_2018.append(line.split(' '))\n",
    "    f_emb.close()\n",
    "    # f_emb_2018.close()\n",
    "\n",
    "\n",
    "    for i in range(len(emb)):\n",
    "        emb[i][len(emb[i])-1] = emb[i][len(emb[i])-1][:-1] # для удаления /n на конце последнего числа эмбеддинга\n",
    "    # for i in range(len(emb_2018)):\n",
    "    #     emb_2018[i][len(emb_2018[i])-1] = emb_2018[i][len(emb_2018[i])-1][:-1]\n",
    "\n",
    "    emb_dict = {} \n",
    "    for i in range(1,len(emb)):\n",
    "        emb_dict[int(emb[i][0])] = [float(j) for j in emb[i][1:]] # собираем эмбеддинг в ключ-значение\n",
    "    # emb_2018_dict = {} \n",
    "    # for i in range(1,len(emb_2018)):\n",
    "    #     emb_2018_dict[int(emb_2018[i][0])] = [float(j) for j in emb_2018[i][1:]]\n",
    "\n",
    "    for e in edges_2019:\n",
    "        if (e[0] in nodes_2019) and (e[1] in nodes_2019):\n",
    "            emb1 = emb_dict[int(e[0])]\n",
    "            emb2 = emb_dict[int(e[1])]\n",
    "            w_19 = int(e[2])\n",
    "            y_test.append(w_19)\n",
    "    #         res = nw_l2(emb1, emb2, Graph_test_2018,int(e[0]),int(e[1]),emb_2018_dict)\n",
    "            res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_train = float(edges_2018_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_train = 0\n",
    "            #res = np.append(res,w_16)\n",
    "            X_test.append(res)\n",
    "\n",
    "    for e in edges_train:\n",
    "        if (e[0] in nodes_train) and (e[1] in nodes_train):\n",
    "            emb1 = emb_dict[int(e[0])]\n",
    "            emb2 = emb_dict[int(e[1])]\n",
    "            w_train = int(e[2])\n",
    "            y_train.append(w_train)\n",
    "    #         res = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\n",
    "            res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_15 = float(edges_2015_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_15 = 0\n",
    "            #res = np.append(res,w_15)\n",
    "            X_train.append(res)\n",
    "    results_train = open('results_train.txt', 'a')\n",
    "    results_test = open('results_test.txt', 'a')\n",
    "    results_train.write(str(operator)+'\\n')\n",
    "    results_test.write(str(operator)+'\\n')\n",
    "    for keycode in range(len(functions)+1)[1:]:\n",
    "        precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = functions.get(int(keycode), unknown_key_pressed)(X_train, y_train, X_test, y_test)\n",
    "        results_train.write(str(precision_train)+ ' ' + str(accuracy_train) + ' ' + str(f1_macro_train) + ' ' + str(f1_micro_train) + ' ' + str(logloss_train) + ' ' + str(roc_auc_train)+ '\\n')\n",
    "        results_test.write(str(precision_test)+ ' ' + str(accuracy_test) + ' ' + str(f1_macro_test) + ' ' + str(f1_micro_test) + ' ' + str(logloss_test) + ' ' + str(roc_auc_test)+ '\\n')\n",
    "    results_train.close()\n",
    "    results_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for operator in range(len(operators_2)+1)[1:]:\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    # f_emb_train = open('./Data/emb_done/out_2018.txt')\n",
    "    f_emb = open('./Data/emb_done/out.txt')\n",
    "    emb = []\n",
    "    # emb_2018 = []\n",
    "    for line in f_emb:\n",
    "        emb.append(line.split(' '))\n",
    "    # for line in f_emb_2018:\n",
    "    #     emb_2018.append(line.split(' '))\n",
    "    f_emb.close()\n",
    "    # f_emb_2018.close()\n",
    "\n",
    "\n",
    "    for i in range(len(emb)):\n",
    "        emb[i][len(emb[i])-1] = emb[i][len(emb[i])-1][:-1] # для удаления /n на конце последнего числа эмбеддинга\n",
    "    # for i in range(len(emb_2018)):\n",
    "    #     emb_2018[i][len(emb_2018[i])-1] = emb_2018[i][len(emb_2018[i])-1][:-1]\n",
    "\n",
    "    emb_dict = {} \n",
    "    for i in range(1,len(emb)):\n",
    "        emb_dict[int(emb[i][0])] = [float(j) for j in emb[i][1:]] # собираем эмбеддинг в ключ-значение\n",
    "    # emb_2018_dict = {} \n",
    "    # for i in range(1,len(emb_2018)):\n",
    "    #     emb_2018_dict[int(emb_2018[i][0])] = [float(j) for j in emb_2018[i][1:]]\n",
    "\n",
    "    for e in edges_2019:\n",
    "        if (e[0] in nodes_2019) and (e[1] in nodes_2019):\n",
    "            emb1 = emb_dict[int(e[0])]\n",
    "            emb2 = emb_dict[int(e[1])]\n",
    "            w_19 = int(e[2])\n",
    "            y_test.append(w_19)\n",
    "            res = operators_2.get(int(operator), unknown_key_pressed)(emb1, emb2, Graph_test_2019,int(e[0]),int(e[1]),emb_dict)\n",
    "#             res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_train = float(edges_2018_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_train = 0\n",
    "            #res = np.append(res,w_16)\n",
    "            X_test.append(res)\n",
    "\n",
    "    for e in edges_train:\n",
    "        if (e[0] in nodes_train) and (e[1] in nodes_train):\n",
    "            emb1 = emb_dict[int(e[0])]\n",
    "            emb2 = emb_dict[int(e[1])]\n",
    "            w_train = int(e[2])\n",
    "            y_train.append(w_train)\n",
    "            res = operators_2.get(int(operator), unknown_key_pressed)(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_dict)\n",
    "#             res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_15 = float(edges_2015_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_15 = 0\n",
    "            #res = np.append(res,w_15)\n",
    "            X_train.append(res)\n",
    "    results_train = open('results_train_2.txt', 'a')\n",
    "    results_test = open('results_test_2.txt', 'a')\n",
    "    results_train.write(str(operator)+'\\n')\n",
    "    results_test.write(str(operator)+'\\n')\n",
    "    for keycode in range(len(functions)+1)[1:]:\n",
    "        precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = functions.get(int(keycode), unknown_key_pressed)(X_train, y_train, X_test, y_test)\n",
    "        results_train.write(str(precision_train)+ ' ' + str(accuracy_train) + ' ' + str(f1_macro_train) + ' ' + str(f1_micro_train) + ' ' + str(logloss_train) + ' ' + str(roc_auc_train)+ '\\n')\n",
    "        results_test.write(str(precision_test)+ ' ' + str(accuracy_test) + ' ' + str(f1_macro_test) + ' ' + str(f1_micro_test) + ' ' + str(logloss_test) + ' ' + str(roc_auc_test)+ '\\n')\n",
    "    results_train.close()\n",
    "    results_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for operator in range(len(operators_4)+1)[1:]:\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    # f_emb_train = open('./Data/emb_done/out_2018.txt')\n",
    "    f_emb = open('./Data/emb_done/embedding_file_LINE.txt')\n",
    "    emb = []\n",
    "    # emb_2018 = []\n",
    "    for line in f_emb:\n",
    "        emb.append(line.split(' '))\n",
    "    # for line in f_emb_2018:\n",
    "    #     emb_2018.append(line.split(' '))\n",
    "    f_emb.close()\n",
    "    # f_emb_2018.close()\n",
    "\n",
    "\n",
    "    for i in range(len(emb)):\n",
    "        emb[i][len(emb[i])-1] = emb[i][len(emb[i])-1][:-1] # для удаления /n на конце последнего числа эмбеддинга\n",
    "    # for i in range(len(emb_2018)):\n",
    "    #     emb_2018[i][len(emb_2018[i])-1] = emb_2018[i][len(emb_2018[i])-1][:-1]\n",
    "\n",
    "    emb_dict = {}\n",
    "    for i in range(1,len(emb)):\n",
    "        emb_dict[int(emb[i][0])] = [float(j) for j in emb[i][1:]] # собираем эмбеддинг в ключ-значение\n",
    "    # emb_2018_dict = {} \n",
    "    # for i in range(1,len(emb_2018)):\n",
    "    #     emb_2018_dict[int(emb_2018[i][0])] = [float(j) for j in emb_2018[i][1:]]\n",
    "\n",
    "    for e in edges_2019:\n",
    "        if (e[0] in nodes_2019) and (e[1] in nodes_2019):\n",
    "            emb1 = emb_dict[int(e[0])]\n",
    "            emb2 = emb_dict[int(e[1])]\n",
    "            w_19 = int(e[2])\n",
    "            y_test.append(w_19)\n",
    "    #         res = nw_l2(emb1, emb2, Graph_test_2018,int(e[0]),int(e[1]),emb_2018_dict)\n",
    "            res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_train = float(edges_2018_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_train = 0\n",
    "            #res = np.append(res,w_16)\n",
    "            X_test.append(res)\n",
    "\n",
    "    for e in edges_train:\n",
    "        if (e[0] in nodes_train) and (e[1] in nodes_train):\n",
    "            emb1 = emb_dict[int(e[0])]\n",
    "            emb2 = emb_dict[int(e[1])]\n",
    "            w_train = int(e[2])\n",
    "            y_train.append(w_train)\n",
    "    #         res = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\n",
    "            res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_15 = float(edges_2015_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_15 = 0\n",
    "            #res = np.append(res,w_15)\n",
    "            X_train.append(res)\n",
    "    results_train = open('results_train_LINE.txt', 'a')\n",
    "    results_test = open('results_test_LINE.txt', 'a')\n",
    "    results_train.write(str(operator)+'\\n')\n",
    "    results_test.write(str(operator)+'\\n')\n",
    "    for keycode in range(len(functions)+1)[1:]:\n",
    "        precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = functions.get(int(keycode), unknown_key_pressed)(X_train, y_train, X_test, y_test)\n",
    "        results_train.write(str(precision_train)+ ' ' + str(accuracy_train) + ' ' + str(f1_macro_train) + ' ' + str(f1_micro_train) + ' ' + str(logloss_train) + ' ' + str(roc_auc_train)+ '\\n')\n",
    "        results_test.write(str(precision_test)+ ' ' + str(accuracy_test) + ' ' + str(f1_macro_test) + ' ' + str(f1_micro_test) + ' ' + str(logloss_test) + ' ' + str(roc_auc_test)+ '\\n')\n",
    "    results_train.close()\n",
    "    results_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABSTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('abstracts_keywords/node_attribute_abstract.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    words = [word.strip(punct) for word in word_tokenize(text.lower())]\n",
    "    text = ' '.join(words)\n",
    "    text = re.sub(r'[\\d+<>\\&\\$\\^@#¯\\(\\)\\{\\}\\[\\]\"©⊗→=\\+μ∞×ℬℳℋπ]', '', text)\n",
    "    text = re.sub(r'[\\\\\\|\\/;]', ' ', text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['abstract_norm'] = data['abstract'].apply(lambda x: normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,2), norm='l2', stop_words=stops, max_features=10000,min_df=0.3, max_df=0.8)\n",
    "vects_tfidf = tfidf.fit_transform(data['abstract_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1058468 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 397 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vects_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_dict_abs_tf = dict(zip(data['node'],vects_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vects_abst = np.load('abstracts_keywords/vects.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46312, 300)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vects_abst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_dict_abs = dict(zip(data['node'],vects_abst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for operator in range(len(operators_4)+1)[1:]:\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for e in edges_2019:\n",
    "        if (e[0] in nodes_2019) and (e[1] in nodes_2019):\n",
    "            emb1 = emd_dict_abs[int(e[0])]\n",
    "            emb2 = emd_dict_abs[int(e[1])]\n",
    "            w_19 = int(e[2])\n",
    "            y_test.append(w_19)\n",
    "    #         res = nw_l2(emb1, emb2, Graph_test_2018,int(e[0]),int(e[1]),emb_2018_dict)\n",
    "            res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_train = float(edges_2018_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_train = 0\n",
    "            #res = np.append(res,w_16)\n",
    "            X_test.append(res)\n",
    "\n",
    "    for e in edges_train:\n",
    "        if (e[0] in nodes_train) and (e[1] in nodes_train):\n",
    "            emb1 = emd_dict_abs[int(e[0])]\n",
    "            emb2 = emd_dict_abs[int(e[1])]\n",
    "            w_train = int(e[2])\n",
    "            y_train.append(w_train)\n",
    "    #         res = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\n",
    "            res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_15 = float(edges_2015_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_15 = 0\n",
    "            #res = np.append(res,w_15)\n",
    "            X_train.append(res)\n",
    "    results_train = open('results_train_abs.txt', 'a')\n",
    "    results_test = open('results_test_abs.txt', 'a')\n",
    "    results_train.write(str(operator)+'\\n')\n",
    "    results_test.write(str(operator)+'\\n')\n",
    "    for keycode in range(len(functions)+1)[1:]:\n",
    "        precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = functions.get(int(keycode), unknown_key_pressed)(X_train, y_train, X_test, y_test)\n",
    "        results_train.write(str(precision_train)+ ' ' + str(accuracy_train) + ' ' + str(f1_macro_train) + ' ' + str(f1_micro_train) + ' ' + str(logloss_train) + ' ' + str(roc_auc_train)+ '\\n')\n",
    "        results_test.write(str(precision_test)+ ' ' + str(accuracy_test) + ' ' + str(f1_macro_test) + ' ' + str(f1_micro_test) + ' ' + str(logloss_test) + ' ' + str(roc_auc_test)+ '\\n')\n",
    "    results_train.close()\n",
    "    results_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for operator in range(len(operators_2)+1)[1:]:\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for e in edges_2019:\n",
    "        if (e[0] in nodes_2019) and (e[1] in nodes_2019):\n",
    "            emb1 = emd_dict_abs[int(e[0])]\n",
    "            emb2 = emd_dict_abs[int(e[1])]\n",
    "            w_19 = int(e[2])\n",
    "            y_test.append(w_19)\n",
    "            res = operators_2.get(int(operator), unknown_key_pressed)(emb1, emb2, Graph_test_2019,int(e[0]),int(e[1]),emd_dict_abs)\n",
    "#             res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_train = float(edges_2018_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_train = 0\n",
    "            #res = np.append(res,w_16)\n",
    "            X_test.append(res)\n",
    "\n",
    "    for e in edges_train:\n",
    "        if (e[0] in nodes_train) and (e[1] in nodes_train):\n",
    "            emb1 = emd_dict_abs[int(e[0])]\n",
    "            emb2 = emd_dict_abs[int(e[1])]\n",
    "            w_train = int(e[2])\n",
    "            y_train.append(w_train)\n",
    "            res = operators_2.get(int(operator), unknown_key_pressed)(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emd_dict_abs)\n",
    "#             res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_15 = float(edges_2015_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_15 = 0\n",
    "            #res = np.append(res,w_15)\n",
    "            X_train.append(res)\n",
    "    results_train = open('results_train_abs_2.txt', 'a')\n",
    "    results_test = open('results_test_abs_2.txt', 'a')\n",
    "    results_train.write(str(operator)+'\\n')\n",
    "    results_test.write(str(operator)+'\\n')\n",
    "    for keycode in range(len(functions)+1)[1:]:\n",
    "        precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = functions.get(int(keycode), unknown_key_pressed)(X_train, y_train, X_test, y_test)\n",
    "        results_train.write(str(precision_train)+ ' ' + str(accuracy_train) + ' ' + str(f1_macro_train) + ' ' + str(f1_micro_train) + ' ' + str(logloss_train) + ' ' + str(roc_auc_train)+ '\\n')\n",
    "        results_test.write(str(precision_test)+ ' ' + str(accuracy_test) + ' ' + str(f1_macro_test) + ' ' + str(f1_micro_test) + ' ' + str(logloss_test) + ' ' + str(roc_auc_test)+ '\\n')\n",
    "    results_train.close()\n",
    "    results_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-b2463d1f20a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#         res = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#         res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_sum_sp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m#         try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#             w_15 = float(edges_2015_dict[(e[0], e[1])])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-212-0508f0fe2f9d>\u001b[0m in \u001b[0;36mavg_sum_sp\u001b[0;34m(v1, v2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mavg_sum_sp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inconsistent shapes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_add_sparse\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_binopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_plus_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sub_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_binopt\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1155\u001b[0m            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m            \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m            indptr, indices, data)\n\u001b[0m\u001b[1;32m   1158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for operator in range(len(operators_4)+1)[1:]:\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for e in edges_2019:\n",
    "    if (e[0] in nodes_2019) and (e[1] in nodes_2019):\n",
    "        emb1 = emd_dict_abs_tf[int(e[0])]\n",
    "        emb2 = emd_dict_abs_tf[int(e[1])]\n",
    "        w_19 = int(e[2])\n",
    "        y_test.append(w_19)\n",
    "#         res = nw_l2(emb1, emb2, Graph_test_2018,int(e[0]),int(e[1]),emb_2018_dict)\n",
    "#         res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "        res = eud_cos(emb1,emb2)\n",
    "#         try:\n",
    "#             w_train = float(edges_2018_dict[(e[0], e[1])])\n",
    "#         except  KeyError :\n",
    "#             w_train = 0\n",
    "        #res = np.append(res,w_16)\n",
    "        X_test.append(res)\n",
    "\n",
    "for e in edges_train:\n",
    "    if (e[0] in nodes_train) and (e[1] in nodes_train):\n",
    "        emb1 = emd_dict_abs_tf[int(e[0])]\n",
    "        emb2 = emd_dict_abs_tf[int(e[1])]\n",
    "        w_train = int(e[2])\n",
    "        y_train.append(w_train)\n",
    "#         res = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\n",
    "#         res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "        res = eud_cos(emb1, emb2)\n",
    "#         try:\n",
    "#             w_15 = float(edges_2015_dict[(e[0], e[1])])\n",
    "#         except  KeyError :\n",
    "#             w_15 = 0\n",
    "        #res = np.append(res,w_15)\n",
    "        X_train.append(res)\n",
    "results_train = open('results_train_abs_tf.txt', 'a')\n",
    "results_test = open('results_test_abs_tf.txt', 'a')\n",
    "# results_train.write(str(operator)+'\\n')\n",
    "# results_test.write(str(operator)+'\\n')\n",
    "for keycode in range(len(functions)+1)[1:]:\n",
    "    precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = functions.get(int(keycode), unknown_key_pressed)(X_train, y_train, X_test, y_test)\n",
    "    results_train.write(str(precision_train)+ ' ' + str(accuracy_train) + ' ' + str(f1_macro_train) + ' ' + str(f1_micro_train) + ' ' + str(logloss_train) + ' ' + str(roc_auc_train)+ '\\n')\n",
    "    results_test.write(str(precision_test)+ ' ' + str(accuracy_test) + ' ' + str(f1_macro_test) + ' ' + str(f1_micro_test) + ' ' + str(logloss_test) + ' ' + str(roc_auc_test)+ '\\n')\n",
    "results_train.close()\n",
    "results_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = emd_dict_abs_tf[7004373913]\n",
    "v2 = emd_dict_abs_tf[55177729700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array((eud[0][0],cos[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.41118452, 0.99572087])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1058468)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = cosine_distances(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99572087]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "eud = euclidean_distances(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kw = pd.read_csv('abstracts_keywords/node_attribute_keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kw = pd.read_csv('abstracts_keywords/node_attribute_keywords_PREP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract_prep</th>\n",
       "      <th>abstract_to_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57021625100</td>\n",
       "      <td>Bioeconomic | Biotechnology | Environmental ec...</td>\n",
       "      <td>['bioeconomic', 'biotechnology', 'environmenta...</td>\n",
       "      <td>bioeconomic biotechnology environmental econom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7004373913</td>\n",
       "      <td>Infrare single-photon detectors | superconduct...</td>\n",
       "      <td>['infrare single-photon detectors', 'supercond...</td>\n",
       "      <td>infrare single-photon detectors superconductin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57202547372</td>\n",
       "      <td>Alcoho | Complexity | Heart rate variability |...</td>\n",
       "      <td>['alcoho', 'complexity', 'heart rate variabili...</td>\n",
       "      <td>alcoho complexity heart rate variability syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55177729700</td>\n",
       "      <td>Coronar heart disease | Dietary inflammatory i...</td>\n",
       "      <td>['coronar heart disease', 'dietary inflammator...</td>\n",
       "      <td>coronar heart disease dietary inflammatory ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27367833300</td>\n",
       "      <td>Boundarie | Prefrontal cortex | Sensorimotor l...</td>\n",
       "      <td>['boundarie', 'prefrontal cortex', 'sensorimot...</td>\n",
       "      <td>boundarie prefrontal cortex sensorimotor learn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          node                                           abstract  \\\n",
       "0  57021625100  Bioeconomic | Biotechnology | Environmental ec...   \n",
       "1   7004373913  Infrare single-photon detectors | superconduct...   \n",
       "2  57202547372  Alcoho | Complexity | Heart rate variability |...   \n",
       "3  55177729700  Coronar heart disease | Dietary inflammatory i...   \n",
       "4  27367833300  Boundarie | Prefrontal cortex | Sensorimotor l...   \n",
       "\n",
       "                                       abstract_prep  \\\n",
       "0  ['bioeconomic', 'biotechnology', 'environmenta...   \n",
       "1  ['infrare single-photon detectors', 'supercond...   \n",
       "2  ['alcoho', 'complexity', 'heart rate variabili...   \n",
       "3  ['coronar heart disease', 'dietary inflammator...   \n",
       "4  ['boundarie', 'prefrontal cortex', 'sensorimot...   \n",
       "\n",
       "                                     abstract_to_vec  \n",
       "0  bioeconomic biotechnology environmental econom...  \n",
       "1  infrare single-photon detectors superconductin...  \n",
       "2  alcoho complexity heart rate variability syste...  \n",
       "3  coronar heart disease dietary inflammatory ind...  \n",
       "4  boundarie prefrontal cortex sensorimotor learn...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Antilocalizatio | Dephasing length | Electron transport | Ferromagnetic state | Magnetic properties | Topological insulator antiferromagnetis | ferromagnetism | magnetic Dipole-dipole interactions | magnetic impurities | platelet inclusions | RKKY interaction | topological insulators Antilocalizatio | Dephasing length | Electron transport | Ferromagnetic state | Magnetic properties | Topological insulator'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kw.abstract[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kw = data_kw.drop(columns=['abstract_prep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kw['abstract_prep']  = data_kw['abstract'].apply(lambda x: re.sub(r'^(\\s\\|\\s)', '', x)).apply(lambda x: x.lower().split(' | '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kw['abstract_prep'] = data_kw['abstract'].apply(lambda x: x.lower().split(' | '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['static analysis', 'strict aliasing', 'type-based alias analysis']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kw['abstract_prep'].iloc[8035]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46312"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_kw['abstract_prep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46312"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(data_kw['abstract_prep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = listmerge3(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listmerge3(lstlst):\n",
    "    all=[]\n",
    "    for lst in lstlst:\n",
    "          all.extend(lst)\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[word for word in line ]for line in vocab_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_done = list(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kw.to_csv('abstracts_keywords/node_attribute_keywords_PREP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kw['abstract_to_vec'] = data_kw['abstract_prep'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,2), norm='l2', stop_words=stops, max_features=10000)\n",
    "vects_tfidf = tfidf.fit_transform(data_kw['abstract_to_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[print(line) for word in line if word==''] for line in vocab_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46312, 59751)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_dict_kw = dict(zip(data_kw['node'],vects_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for operator in range(len(operators_4)+1)[1:]:\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for e in edges_2019:\n",
    "        if (e[0] in nodes_2019) and (e[1] in nodes_2019):\n",
    "            emb1 = emd_dict_kw[int(e[0])]\n",
    "            emb2 = emd_dict_kw[int(e[1])]\n",
    "            w_19 = int(e[2])\n",
    "            y_test.append(w_19)\n",
    "    #         res = nw_l2(emb1, emb2, Graph_test_2018,int(e[0]),int(e[1]),emb_2018_dict)\n",
    "            res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_train = float(edges_2018_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_train = 0\n",
    "            #res = np.append(res,w_16)\n",
    "            X_test.append(res)\n",
    "\n",
    "    for e in edges_train:\n",
    "        if (e[0] in nodes_train) and (e[1] in nodes_train):\n",
    "            emb1 = emd_dict_kw[int(e[0])]\n",
    "            emb2 = emd_dict_kw[int(e[1])]\n",
    "            w_train = int(e[2])\n",
    "            y_train.append(w_train)\n",
    "    #         res = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\n",
    "            res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_15 = float(edges_2015_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_15 = 0\n",
    "            #res = np.append(res,w_15)\n",
    "            X_train.append(res)\n",
    "    results_train = open('results_train_kw.txt', 'a')\n",
    "    results_test = open('results_test_kw.txt', 'a')\n",
    "    results_train.write(str(operator)+'\\n')\n",
    "    results_test.write(str(operator)+'\\n')\n",
    "    for keycode in range(len(functions)+1)[1:]:\n",
    "        precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = functions.get(int(keycode), unknown_key_pressed)(X_train, y_train, X_test, y_test)\n",
    "        results_train.write(str(precision_train)+ ' ' + str(accuracy_train) + ' ' + str(f1_macro_train) + ' ' + str(f1_micro_train) + ' ' + str(logloss_train) + ' ' + str(roc_auc_train)+ '\\n')\n",
    "        results_test.write(str(precision_test)+ ' ' + str(accuracy_test) + ' ' + str(f1_macro_test) + ' ' + str(f1_micro_test) + ' ' + str(logloss_test) + ' ' + str(roc_auc_test)+ '\\n')\n",
    "    results_train.close()\n",
    "    results_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for operator in range(len(operators_2)+1)[1:]:\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for e in edges_2019:\n",
    "        if (e[0] in nodes_2019) and (e[1] in nodes_2019):\n",
    "            emb1 = emd_dict_kw[int(e[0])]\n",
    "            emb2 = emd_dict_kw[int(e[1])]\n",
    "            w_19 = int(e[2])\n",
    "            y_test.append(w_19)\n",
    "            res = operators_2.get(int(operator), unknown_key_pressed)(emb1, emb2, Graph_test_2019,int(e[0]),int(e[1]),emd_dict_kw)\n",
    "#             res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_train = float(edges_2018_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_train = 0\n",
    "            #res = np.append(res,w_16)\n",
    "            X_test.append(res)\n",
    "\n",
    "    for e in edges_train:\n",
    "        if (e[0] in nodes_train) and (e[1] in nodes_train):\n",
    "            emb1 = emd_dict_kw[int(e[0])]\n",
    "            emb2 = emd_dict_kw[int(e[1])]\n",
    "            w_train = int(e[2])\n",
    "            y_train.append(w_train)\n",
    "            res = operators_2.get(int(operator), unknown_key_pressed)(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emd_dict_kw)\n",
    "#             res = operators_4.get(int(operator), unknown_key_pressed)(emb1, emb2)\n",
    "    #         try:\n",
    "    #             w_15 = float(edges_2015_dict[(e[0], e[1])])\n",
    "    #         except  KeyError :\n",
    "    #             w_15 = 0\n",
    "            #res = np.append(res,w_15)\n",
    "            X_train.append(res)\n",
    "    results_train = open('results_train_kw_2.txt', 'a')\n",
    "    results_test = open('results_test_kw_2.txt', 'a')\n",
    "    results_train.write(str(operator)+'\\n')\n",
    "    results_test.write(str(operator)+'\\n')\n",
    "    for keycode in range(len(functions)+1)[1:]:\n",
    "        precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = functions.get(int(keycode), unknown_key_pressed)(X_train, y_train, X_test, y_test)\n",
    "        results_train.write(str(precision_train)+ ' ' + str(accuracy_train) + ' ' + str(f1_macro_train) + ' ' + str(f1_micro_train) + ' ' + str(logloss_train) + ' ' + str(roc_auc_train)+ '\\n')\n",
    "        results_test.write(str(precision_test)+ ' ' + str(accuracy_test) + ' ' + str(f1_macro_test) + ' ' + str(f1_micro_test) + ' ' + str(logloss_test) + ' ' + str(roc_auc_test)+ '\\n')\n",
    "    results_train.close()\n",
    "    results_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
